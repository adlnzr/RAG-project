{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install llama-index-readers-file\n",
    "# %pip install llama-index-embeddings-huggingface\n",
    "# %pip install llama-index-llms-huggingface\n",
    "# %pip install llama-index-llms-llama-cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"transformers[torch]\" \"huggingface_hub[inference]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python  --no-cache-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence transformers\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeded_text = embed_model.get_text_embedding(\"hello Mapsa Iran Tehran!\")\n",
    "len(embeded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HUGGING_FACE_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/t7/rf1zzxt969xddtxvl6f_hcsm0000gn/T/ipykernel_43006/850126269.py:8: DeprecationWarning: Call to deprecated class HuggingFaceInferenceAPI. (Deprecated in favor of `HuggingFaceInferenceAPI` from `llama-index-llms-huggingface-api` which should be used instead.)\n",
      "  remotely_run = HuggingFaceInferenceAPI(\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.huggingface import (\n",
    "    HuggingFaceInferenceAPI,\n",
    "    HuggingFaceLLM,\n",
    ")\n",
    "\n",
    "# locally_run = HuggingFaceLLM(model_name=\"HuggingFaceH4/zephyr-7b-beta\")\n",
    "\n",
    "remotely_run = HuggingFaceInferenceAPI(\n",
    "    model_name=\"HuggingFaceH4/zephyr-7b-beta\", token=HF_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionResponse(text=\"\\n\\nI'm planning to visit some places in Paris and I'm not familiar with the public transportation system. I'm wondering which one would be the most convenient and efficient way to travel between these places.\\n\\nHere are the places I want to visit:\\n\\n1. Eiffel Tower\\n2. Louvre Museum\\n3. Notre Dame Cathedral\\n4. Montmartre\\n5. Sacré-Cœur Basilica\\n6. Champs-Élysées\\n7. Arc de Triomphe\\n8. Opera Garnier\\n9. Saint-Germain-des-Prés\\n10. Luxembourg Gardens\\n\\nI'm looking for a transportation option that is easy to use, frequent, and covers most of these places. Which one would you recommend?\\n\\nAlso, do you have any tips on how to navigate the public transportation system in Paris? I'm a bit overwhelmed by the number of lines and connections.\", additional_kwargs={}, raw=None, logprobs=None, delta=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remotely_run.complete(\"what would be best public transportation solution between tramway, bus and subway?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Package(s) not found: llama-index\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip show llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader, Document\n",
    "\n",
    "documents = SimpleDirectoryReader(\n",
    "    \"./data/\"\n",
    ").load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceWindowNodeParser\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "\n",
    "# create the sentence window node parser w/ default settings\n",
    "node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=3,\n",
    "    window_metadata_key=\"window\",\n",
    "    original_text_metadata_key=\"original_text\",\n",
    ")\n",
    "\n",
    "# base node parser is a sentence splitter\n",
    "text_splitter = SentenceSplitter()\n",
    "\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = remotely_run\n",
    "Settings.embed_model = embed_model\n",
    "Settings.text_splitter = text_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The best public transportation solution would depend on the specific location and the needs of the individual. However, based on the provided context, the tramway seems to have some advantages over bus and subway. \n",
      "\n",
      "Firstly, the tramway is expected to carry 10 million passengers per year, which is significantly higher than the current bus usage of 6.5 million passengers per year. This suggests that the tramway may provide a more efficient and reliable service.\n",
      "\n",
      "Secondly, some 35% of the increased passenger traffic is expected to transfer from cars, which could help to reduce congestion and air pollution in the area. This is in contrast to the subway, which may not have the same level of transfer potential.\n",
      "\n",
      "Thirdly, the tramway is expected to be fully accessible to the mobility impaired, which is an important consideration for many people. This is not necessarily the case for all bus services, and the subway may also have some accessibility issues.\n",
      "\n",
      "Fourthly, the tramway fares are expected to be slightly more than existing bus fares, but still competitive. This could make it an attractive option for people who are currently driving their cars.\n",
      "\n",
      "Finally,\n"
     ]
    }
   ],
   "source": [
    "node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "    window_size=3,\n",
    "    window_metadata_key=\"window\",\n",
    "    original_text_metadata_key=\"original_text\",\n",
    ")\n",
    "\n",
    "nodes = node_parser.get_nodes_from_documents(documents)  #len: 10290\n",
    "base_nodes = text_splitter.get_nodes_from_documents(documents) #len: 743\n",
    "\n",
    "Settings.llm = remotely_run\n",
    "Settings.embed_model = embed_model\n",
    "Settings.text_splitter = text_splitter\n",
    "\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "sentence_index = VectorStoreIndex(nodes)\n",
    "# base_index = VectorStoreIndex(base_nodes)\n",
    "\n",
    "from llama_index.core.postprocessor import MetadataReplacementPostProcessor\n",
    "\n",
    "query_engine = sentence_index.as_query_engine(\n",
    "    similarity_top_k=2,\n",
    "    # the target key defaults to `window` to match the node_parser's default\n",
    "    node_postprocessors=[\n",
    "        MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
    "    ],\n",
    ")\n",
    "window_response = query_engine.query(\n",
    "    \"what would be best public transportation solution between tramway, bus and subway? \"\n",
    ")\n",
    "\n",
    "print(window_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
